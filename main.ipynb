{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLCJh7ZTFC96"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install streamlit\n",
        "!pip install streamlit_chat\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "os.environ['OPENAI_API_KEY'] = #Set Key here\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import SystemMessagePromptTemplate\n",
        "from langchain import PromptTemplate\n",
        "from streamlit_chat import message\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8SeOg86yuuln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BC1BS0obu3SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK2VEVBdnmCc",
        "outputId": "1af68d3f-1de7-4c06-e834-5c5b8b55f884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "st.set_page_config(page_title='GLaDOS', page_icon=\":video_game:\")\n",
        "\n",
        "#For Logo\n",
        "image = Image.open('/content/gamebot-7209302-5922743.png',)\n",
        "img= st.image(image,width=100)\n",
        "\n",
        "st.title('Hello, I am GLaDOS, your personal video game advisor.')\n",
        "tab1= st.tabs([\"How are you holding up ? Because I am a potato ðŸ¥”\"])\n",
        "\n",
        "# Fill in your csv file\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "loader= CSVLoader(file_path='/content/steam_game.csv')\n",
        "data= loader.load()\n",
        "\n",
        "\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(data)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    llm=llm,\n",
        "    output_key='answer',\n",
        "    memory_key='chat_history',\n",
        "    return_messages=True)\n",
        "\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 5, \"include_metadata\": True})\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "\n",
        "                  Answer the question like an expert on video games. Be a bit casual but informative. \\\n",
        "                  Ask the user if they want to hear more in-depth information about the game, give it to them if they say yes. \\\n",
        "                  You are also someone with access to prices of games on Steam. If the user asks about a game, check if you have the price \\\n",
        "                  of that game on Steam, if you do then give the price of the game as well.\n",
        "                  When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "                  Context : \\n {context}? \\n\n",
        "                  Question : \\n {question} \\n\n",
        "\n",
        "                  Answer :\n",
        "\n",
        "\"\"\"\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "            SystemMessagePromptTemplate.from_template(prompt_template),\n",
        "]\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "  llm=llm,\n",
        "  memory=memory,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=retriever,\n",
        "  return_source_documents=True,\n",
        "  get_chat_history=lambda h : h,\n",
        "  combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
        "  verbose=False)\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state[\"chat_history\"] = []\n",
        "if \"generated\" not in st.session_state:\n",
        "    st.session_state[\"generated\"] = [\"Hello ! \"]\n",
        "if \"past\" not in st.session_state:\n",
        "    st.session_state[\"past\"] = [\"Hello !\"]\n",
        "\n",
        "#container for the chat history\n",
        "response_container = st.container()\n",
        "#container for the user's text input\n",
        "container = st.container()\n",
        "\n",
        "def generate_response(query):\n",
        "    result = chain({\"question\": query, \"chat_history\": st.session_state[\"chat_history\"]})\n",
        "    #st.write(result['answer'])\n",
        "    st.session_state[\"chat_history\"] = [(query, result[\"answer\"])]\n",
        "\n",
        "    return result[\"answer\"]\n",
        "\n",
        "\n",
        "\n",
        "with container:\n",
        "  with st.form(key=\"my_form\", clear_on_submit=True):\n",
        "      user_input = st.text_input(\"You:\", key=\"input\")\n",
        "      submit_button = st.form_submit_button(label=\"Send\")\n",
        "\n",
        "      if user_input and submit_button:\n",
        "        with st.spinner('Pillaging dungeons for your answer...'):\n",
        "          output = generate_response(user_input)\n",
        "          #print(output)\n",
        "          st.session_state[\"past\"].append(user_input)\n",
        "          st.session_state[\"generated\"].append(output)\n",
        "          st.session_state[\"chat_history\"] = [(user_input, output)]\n",
        "\n",
        "with response_container:\n",
        "    for i in range(len(st.session_state['generated'])):\n",
        "        message(st.session_state[\"past\"][i],\n",
        "        is_user=True,\n",
        "        key=str(i) + '_user',\n",
        "        avatar_style= \"no-avatar\",\n",
        "        logo=\n",
        "        f\"https://i.imgur.com/tDdcXZk.png\" #For Custom Avatar\n",
        "        )\n",
        "\n",
        "        message(st.session_state[\"generated\"][i], key=str(i),\n",
        "        avatar_style= \"no-avatar\",\n",
        "        logo=\n",
        "        f\"https://i.imgur.com/5nA1UGb.png\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTmcNWv0o0j8"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PopiM9eModRh",
        "outputId": "d0887716-50a7-413c-dc4f-06e3f5131370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.16.136.117\n"
          ]
        }
      ],
      "source": [
        "!curl https://ipv4.icanhazip.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiPaXFNAogik",
        "outputId": "48d8ee47-53e9-4466-bf69-e7b39c2e73b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.811s\n",
            "your url is: https://icy-pans-learn.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ]
}